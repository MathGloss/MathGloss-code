{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class WikiMapper:\n",
    "    \"\"\"Uses a precomputed database created by `create_wikipedia_wikidata_mapping_db`.\"\"\"\n",
    "\n",
    "    def __init__(self, path_to_db: str):\n",
    "        self._path_to_db = path_to_db\n",
    "        self.conn = sqlite3.connect(self._path_to_db)\n",
    "\n",
    "    def title_to_id(self, page_title: str) -> Optional[str]:\n",
    "        \"\"\"Given a Wikipedia page title, returns the corresponding Wikidata ID.\n",
    "        The page title is the last part of a Wikipedia url **unescaped** and spaces\n",
    "        replaced by underscores , e.g. for `https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem`,\n",
    "        the title would be `Fermat's_Last_Theorem`.\n",
    "        Args:\n",
    "            page_title: The page title of the Wikipedia entry, e.g. `Manatee`.\n",
    "        Returns:\n",
    "            Optional[str]: If a mapping could be found for `wiki_page_title`, then return\n",
    "                           it, else return `None`.\n",
    "        \"\"\"\n",
    "\n",
    "        c = self.conn.execute(\"SELECT wikidata_id FROM mapping WHERE wikipedia_title=?\", (page_title,))\n",
    "        result = c.fetchone()\n",
    "\n",
    "        if result is not None and result[0] is not None:\n",
    "            return result[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def url_to_id(self, wiki_url: str) -> Optional[str]:\n",
    "        \"\"\"Given an URL to a Wikipedia page, returns the corresponding Wikidata ID.\n",
    "        This is just a convenience function. It is not checked whether the index and\n",
    "        URL are from the same dump.\n",
    "        Args:\n",
    "            wiki_url: The URL to a Wikipedia entry.\n",
    "        Returns:\n",
    "            Optional[str]: If a mapping could be found for `wiki_url`, then return\n",
    "                           it, else return `None`.\n",
    "        \"\"\"\n",
    "\n",
    "        title = wiki_url.rsplit(\"/\", 1)[-1]\n",
    "        return self.title_to_id(title)\n",
    "\n",
    "    def id_to_titles(self, wikidata_id: str) -> List[str]:\n",
    "        \"\"\"Given a Wikidata ID, return a list of corresponding pages that are linked to it.\n",
    "        Due to redirects, the mapping from Wikidata ID to Wikipedia title is not unique.\n",
    "        Args:\n",
    "            wikidata_id (str): The Wikidata ID to map, e.g. `Q42797`.\n",
    "        Returns:\n",
    "            List[str]: A list of Wikipedia pages that are linked to this Wikidata ID.\n",
    "        \"\"\"\n",
    "\n",
    "        c = self.conn.execute(\n",
    "            \"SELECT DISTINCT wikipedia_title FROM mapping WHERE wikidata_id =?\", (wikidata_id,)\n",
    "        )\n",
    "        results = c.fetchall()\n",
    "\n",
    "        return [e[0] for e in results]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHICAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/index_enwiki-20190420.db')\n",
    "\n",
    "definitions = '/Users/lucyhorowitz/Library/Mobile Documents/iCloud~md~obsidian/Documents/MathRepository/notes/definitions'\n",
    "\n",
    "wiki = open('chitest.txt', 'a')\n",
    "towrite = []\n",
    "for filepath in os.listdir(definitions):\n",
    "    defn = os.path.join(definitions, filepath)\n",
    "    defn_text = open(defn).read()\n",
    "    \n",
    "    ids = [url[30:] for url in defn_text.split() if url.__contains__('wikidata')]\n",
    "    if ids:\n",
    "        for thing in ids:\n",
    "            towrite.append(thing + ': ' + filepath[:-3] + '\\n')\n",
    "        #wiki.write('\\n')\n",
    "\n",
    "for thing in towrite:       \n",
    "    wiki.write(thing)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROOFNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofnet is small, so no cutoff\n",
    "\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "\n",
    "wikicats = ['_(mathematics)', '_(linear_algebra)', '_(algebraic_geometry)', '_(algebraic_topology)', '_(calculus)', '_(category_theory)',\n",
    "             '_(commutative_algebra)', '_(field_theory)', '_(game_theory)', '_(topology)', '_(differential_geometry)', '_(graph_theory)', \n",
    "             '_(group_theory)', '_(invariant_theory)', '_(module_theory)', '_(order_theory)', '_(probability)', '_(statistics)', '_(ring_theory)',\n",
    "             '_(representation_theory)', '_(set_theory)', '_(string_theory)', '_(symplectic geometry)', '_(tensor_theory)']\n",
    "\n",
    "wiki = open('pn-plus.txt', 'a')\n",
    "#wiki.write('ADJ-NOUN PAIRS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/proofnet/pn-ans.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:line.index(':')]\n",
    "    linesplus = [line + cat for cat in wikicats]\n",
    "    exclude = False\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            exclude = True\n",
    "    if not exclude:\n",
    "        linesplus.append(line)\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            wiki.write(thing + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "\n",
    "#wiki.write('\\nCOMPOUNDS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/proofnet/pn-compounds.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:line.index(':')]\n",
    "    linesplus = [line + cat for cat in wikicats]\n",
    "    exclude = False\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            exclude = True\n",
    "    if not exclude:\n",
    "        linesplus.append(line)\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            wiki.write(thing + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "\n",
    "#wiki.write('\\nNOUNS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/proofnet/pn-nouns.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:line.index(':')]\n",
    "    linesplus = [line + cat for cat in wikicats]\n",
    "    exclude = False\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            exclude = True\n",
    "    if not exclude:\n",
    "        linesplus.append(line)\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            wiki.write(thing + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "#wiki.write('\\nNOUNS:\\n')\n",
    "#for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/tac /hyph-propns.txt', 'r').readlines():\n",
    "    #line = line.replace(' - ', '-')\n",
    "    #line = line.replace('$', '')\n",
    "    #line = line.replace(' ', '_')\n",
    "    #line = line[0].upper() + line[1:line.index(':')]\n",
    "    #if map.title_to_id(line):\n",
    "        #wiki.write(line + ': ' + map.title_to_id(line)  + '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n",
      "ehllo\n"
     ]
    }
   ],
   "source": [
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "\n",
    "wikicats = ['_(mathematics)', '_(linear_algebra)', '_(algebraic_geometry)', '_(algebraic_topology)', '_(calculus)', '_(category_theory)',\n",
    "             '_(commutative_algebra)', '_(field_theory)', '_(game_theory)', '_(topology)', '_(differential_geometry)', '_(graph_theory)', \n",
    "             '_(group_theory)', '_(invariant_theory)', '_(module_theory)', '_(order_theory)', '_(probability)', '_(ring_theory)',\n",
    "             '_(representation_theory)', '_(set_theory)', '_(string_theory)', '_(symplectic geometry)', '_(tensor_theory)']\n",
    "\n",
    "wiki = open('france-4.txt', 'a')\n",
    "#wiki.write('\\nIN LEAN:\\n')\n",
    "for text in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/france/france.txt', 'r').readlines():\n",
    "    line = text.split(': ')[1].strip()\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:].strip()\n",
    "    linesplus = [line + cat for cat in wikicats]\n",
    "    exclude = False\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            exclude = True\n",
    "    if not exclude:\n",
    "        linesplus.append(line)\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            wiki.write(thing.replace('_', \" \") + \"](\" + text.split(': ')[0].strip() + '): ' + map.title_to_id(thing) +'\\n')\n",
    "\n",
    "#wiki.write('\\nNOT IN LEAN:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/france/france-no-lean.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:].strip()\n",
    "    linesplus = [line + cat for cat in wikicats]\n",
    "    exclude = False\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            exclude = True\n",
    "    if not exclude:\n",
    "        linesplus.append(line)\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            wiki.write(thing.replace('_', ' ') + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "            print('ehllo')\n",
    "\n",
    "#wiki.write('\\nNOUNS:\\n')\n",
    "#for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/tac /hyph-propns.txt', 'r').readlines():\n",
    "    #line = line.replace(' - ', '-')\n",
    "    #line = line.replace('$', '')\n",
    "    #line = line.replace(' ', '_')\n",
    "    #line = line[0].upper() + line[1:line.index(':')]\n",
    "    #if map.title_to_id(line):\n",
    "        #wiki.write(line + ': ' + map.title_to_id(line)  + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tac cutoff is n = 5\n",
    "\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "\n",
    "wikicats = ['_(mathematics)', '_(linear_algebra)', '_(algebraic_geometry)', '_(algebraic_topology)', '_(calculus)', '_(category_theory)',\n",
    "             '_(commutative_algebra)', '_(field_theory)', '_(game_theory)', '_(topology)', '_(differential_geometry)', '_(graph_theory)', \n",
    "             '_(group_theory)', '_(invariant_theory)', '_(module_theory)', '_(order_theory)', '_(probability)', '_(statistics)', '_(ring_theory)',\n",
    "             '_(representation_theory)', '_(set_theory)', '_(string_theory)', '_(symplectic geometry)', '_(tensor_theory)']\n",
    "\n",
    "wiki = open('tac-plus.txt', 'a')\n",
    "#wiki.write('ADJ-NOUN PAIRS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/tac/hyph-an-counts.txt', 'r').readlines():\n",
    "    if int(line[line.index(':') + 2]) >= 5:\n",
    "        line = line.replace(' - ', '-')\n",
    "        line = line.replace('$', '')\n",
    "        line = line.replace(' ', '_')\n",
    "        line = line[0].upper() + line[1:line.index(':')]\n",
    "        linesplus = [line + cat for cat in wikicats]\n",
    "        exclude = False\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                exclude = True\n",
    "        if not exclude:\n",
    "            linesplus.append(line)\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                wiki.write(thing + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "                #print(thing)\n",
    "\n",
    "#wiki.write('\\nCOMPOUNDS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/tac/hyph-compounds.txt', 'r').readlines():\n",
    "    if int(line[line.index(':') + 2]) >= 5:\n",
    "        line = line.replace(' - ', '-')\n",
    "        line = line.replace('$', '')\n",
    "        line = line.replace(' ', '_')\n",
    "        line = line[0].upper() + line[1:line.index(':')]\n",
    "        linesplus = [line + cat for cat in wikicats]\n",
    "        exclude = False\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                exclude = True\n",
    "        if not exclude:\n",
    "            linesplus.append(line)\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                wiki.write(thing + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "                #print(thing)\n",
    "\n",
    "#wiki.write('\\nNOUNS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/tac/hyph-nouns.txt', 'r').readlines():\n",
    "   if int(line[line.index(':') + 2]) >= 5:\n",
    "        line = line.replace(' - ', '-')\n",
    "        line = line.replace('$', '')\n",
    "        line = line.replace(' ', '_')\n",
    "        line = line[0].upper() + line[1:line.index(':')]\n",
    "        linesplus = [line + cat for cat in wikicats]\n",
    "        exclude = False\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                exclude = True\n",
    "        if not exclude:\n",
    "            linesplus.append(line)\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                wiki.write(thing + ': ' + map.title_to_id(thing)  + '\\n')\n",
    "                #print(thing)\n",
    "\n",
    "#wiki.write('\\nNOUNS:\\n')\n",
    "#for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/tac /hyph-propns.txt', 'r').readlines():\n",
    "    #line = line.replace(' - ', '-')\n",
    "    #line = line.replace('$', '')\n",
    "    #line = line.replace(' ', '_')\n",
    "    #line = line[0].upper() + line[1:line.index(':')]\n",
    "    #if map.title_to_id(line):\n",
    "        #wiki.write(line + ': ' + map.title_to_id(line)  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make nlab terms into one big list\n",
    "termlist = []\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/nlab/nlab-ans.txt', 'r').readlines():\n",
    "    termlist.append(line[:line.index(':')])\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/nlab/nlab-compounds.txt', 'r').readlines():\n",
    "    termlist.append(line[:line.index(':')])\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/nlab/nlab-nouns.txt', 'r').readlines():\n",
    "    termlist.append(line[:line.index(':')])\n",
    "#for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/nlab/nlab-propns.txt', 'r').readlines():\n",
    "    #termlist.append(line[:line.index(':')])\n",
    "\n",
    "termlist = sorted(set(termlist))\n",
    "\n",
    "with open('nlab-terms.txt', 'a') as writer:\n",
    "    for term in termlist:\n",
    "        writer.write(term + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tac-term-mappings.txt','a') as mappings:\n",
    "    listy = []\n",
    "    for line in open('tac-terms.txt','r').readlines():\n",
    "        line = line.replace(' - ', '-')\n",
    "        line = line.replace('$', '')\n",
    "        line = line.replace(' ', '_')\n",
    "        line = line[0].upper() + line[1:-1]\n",
    "        if map.title_to_id(line):\n",
    "            listy.append(line + ': ' + map.title_to_id(line)  + '\\n')\n",
    "\n",
    "    listy= sorted(set(listy))\n",
    "    for line in listy:\n",
    "        mappings.write(line)        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MISSING terms from wikidata\n",
    "#term-id order, for tac stuff\n",
    "\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "\n",
    "wiki = open('france-missing.txt', 'a')\n",
    "#wiki.write('ADJ-NOUN PAIRS:\\n')\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/france/france.txt', 'r').readlines():\n",
    "    line2 = line.split(', ')[1].strip()\n",
    "    line2 = line2.replace(' - ', '-')\n",
    "    line2 = line2.replace('$', '')\n",
    "    line2 = line2.replace(' ', '_')\n",
    "    line2 = line2[0].upper() + line2[1:]\n",
    "    if not map.title_to_id(line2):\n",
    "        wiki.write('- ' + line2 + '\\n')\n",
    "\n",
    "for line in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/france/france-non-lean.txt','r').readlines():\n",
    "    line = line.strip()\n",
    "    line2 = line.replace(' - ', '-')\n",
    "    line2 = line2.replace('$', '')\n",
    "    line2 = line2.replace(' ', '_')\n",
    "    line2 = line2[0].upper() + line2[1:]\n",
    "    if not map.title_to_id(line2):\n",
    "        wiki.write('- '+ line2 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such table: mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     line \u001b[39m=\u001b[39m line[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mupper() \u001b[39m+\u001b[39m line[\u001b[39m1\u001b[39m:line\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mmap\u001b[39;49m\u001b[39m.\u001b[39;49mtitle_to_id(line):\n\u001b[1;32m     13\u001b[0m         dictionary[line] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m\u001b[39m.\u001b[39mtitle_to_id(line) \n\u001b[1;32m     14\u001b[0m         \u001b[39m#wiki.write(map.title_to_id(line) + ': ' + line  + '\\n')\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m#wiki.write('\\nCOMPOUNDS:\\n')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mWikiMapper.title_to_id\u001b[0;34m(self, page_title)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path_to_db) \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m     28\u001b[0m     c \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m---> 29\u001b[0m     c\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT wikidata_id FROM mapping WHERE wikipedia_title=?\u001b[39;49m\u001b[39m\"\u001b[39;49m, (page_title,))\n\u001b[1;32m     30\u001b[0m     result \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mfetchone()\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m result[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: mapping"
     ]
    }
   ],
   "source": [
    "#id-term order, for tac stuff\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/index_enwiki-20190420.db')\n",
    "\n",
    "dictionary = dict()\n",
    "wiki = open('tac-wikidata.txt', 'a')\n",
    "#wiki.write('ADJ-NOUN PAIRS:\\n')\n",
    "for line in open('hf-ans.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:line.index(':')]\n",
    "    if map.title_to_id(line):\n",
    "        dictionary[line] = map.title_to_id(line) \n",
    "        #wiki.write(map.title_to_id(line) + ': ' + line  + '\\n')\n",
    "\n",
    "#wiki.write('\\nCOMPOUNDS:\\n')\n",
    "for line in open('hf-compounds.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:line.index(':')]\n",
    "    if map.title_to_id(line):\n",
    "        dictionary[line] = map.title_to_id(line) \n",
    "        #wiki.write(map.title_to_id(line) + ': ' + line  + '\\n')\n",
    "\n",
    "#wiki.write('\\nPROPER NOUNS:\\n')\n",
    "#for line in open('hf-propns.txt', 'r').readlines():\n",
    "    #line = line.replace('$', '')\n",
    "    #line = line.replace(' ', '_')\n",
    "    #line = line[0].upper() + line[1:line.index(':')]\n",
    "    #if map.title_to_id(line):\n",
    "        #dictionary[line] = map.title_to_id(line) \n",
    "        #wiki.write(map.title_to_id(line) + ': ' + line  + '\\n')\n",
    "\n",
    "#wiki.write('\\nNOUNS:\\n')\n",
    "for line in open('hf-nouns.txt', 'r').readlines():\n",
    "    line = line.replace(' - ', '-')\n",
    "    line = line.replace('$', '')\n",
    "    line = line.replace(' ', '_')\n",
    "    line = line[0].upper() + line[1:line.index(':')]\n",
    "    if map.title_to_id(line):\n",
    "        dictionary[line] = map.title_to_id(line) \n",
    "        #wiki.write(map.title_to_id(line) + ': ' + line  + '\\n')\n",
    "\n",
    "dictionary2 = sorted(dictionary)\n",
    "print(dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this needs to be run twice to get past W for some reason, then just delete the repeated things.\n",
    "\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/index_enwiki-20190420.db')\n",
    "\n",
    "wiki = open('planetmath-wikidata.txt', 'a')\n",
    "toread = open('planetmath.txt', 'r').readlines()\n",
    "#toread = ['Cylindrical Coordinates\\n', 'Cylindrical Coordinate System\\n']\n",
    "towrite = []\n",
    "for line in toread:\n",
    "    line = line[:-1]\n",
    "    if map.title_to_id(line):\n",
    "        towrite.append(line  +': ' +map.title_to_id(line) + '\\n')\n",
    "    else:\n",
    "        line = line.replace(' - ', '-')\n",
    "        line = line.replace('$', '')\n",
    "        line = line.replace(' ', '_')\n",
    "        if map.title_to_id(line):\n",
    "            towrite.append(line  +': ' +map.title_to_id(line) + '\\n')\n",
    "        else:\n",
    "            line = line.lower()\n",
    "            line = line[0].upper() + line[1:]\n",
    "            if map.title_to_id(line):\n",
    "                towrite.append(line  +': ' +map.title_to_id(line) + '\\n')\n",
    "\n",
    "to = sorted(towrite)\n",
    "for thing in to:\n",
    "    wiki.write(thing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = open('planetmath.txt','r').readlines()\n",
    "moon = []\n",
    "for line in planet:\n",
    "    for term in line.split():\n",
    "        moon.append(term)\n",
    "\n",
    "moon2 = []\n",
    "for term in moon:\n",
    "    if term[0].isnumeric(): #remove leading numbers\n",
    "        i = 0\n",
    "        while term[i].isnumeric():\n",
    "            term = term[i+1:]\n",
    "    i = 0\n",
    "    for letter in term:\n",
    "        if i > 0 and term[i].isupper():\n",
    "            term = term[:i] + '_' + term[i:]\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "    moon2.append(term)\n",
    "\n",
    "moon2 = sorted(moon2)\n",
    "with open('planetmath-clean.txt', 'a') as mon:\n",
    "    for term in moon2:\n",
    "        mon.write(term + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n"
     ]
    }
   ],
   "source": [
    "#for france\n",
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "france = open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/france/france.txt','r').readlines()\n",
    "towrite = []\n",
    "for line in france:\n",
    "    twos = line.split(', ')\n",
    "    twos[1] = twos[1].replace(' - ', '-')\n",
    "    twos[1] = twos[1][0].upper() + twos[1][1:].strip()\n",
    "    twos[1] =  twos[1].replace('$', '')\n",
    "    twos[1]= twos[1].replace(' ', '_')\n",
    "    if twos[1][-1] == 's':\n",
    "        twos[1] = twos[1][:-1]\n",
    "    if map.title_to_id(twos[1]):\n",
    "        towrite.append(map.title_to_id(twos[1]) + ': ' + twos[1] + '\\n')\n",
    "wiki = open('france-mappings.txt', 'a')\n",
    "for thing in towrite:\n",
    "    wiki.write(thing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/chicago defs/defs.txt','r').readlines()\n",
    "all = [term.strip() for term in all]\n",
    "\n",
    "mappings = open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/chicago defs/manual-def-mappings.txt','r').readlines()\n",
    "\n",
    "terms = []\n",
    "for term in mappings:\n",
    "    data = term.split(': ')\n",
    "    terms.append(data[1].strip())\n",
    "\n",
    "\n",
    "with open('chicago-missing.txt','a') as missing:\n",
    "    for term in all:\n",
    "        if not term in terms:\n",
    "            missing.write(\"- \" + term + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "\n",
    "wikicats = ['_(mathematics)', '_(linear_algebra)', '_(algebraic_geometry)', '_(algebraic_topology)', '_(calculus)', '_(category_theory)',\n",
    "             '_(commutative_algebra)', '_(field_theory)', '_(game_theory)', '_(topology)', '_(differential_geometry)', '_(graph_theory)', \n",
    "             '_(group_theory)', '_(invariant_theory)', '_(module_theory)', '_(order_theory)', '_(probability)', '_(ring_theory)',\n",
    "             '_(representation_theory)', '_(set_theory)', '_(string_theory)', '_(symplectic geometry)', '_(tensor_theory)']\n",
    "\n",
    "wiki = open('nlab-maps2.txt', 'a')\n",
    "\n",
    "people = [name.strip() for name in open(\"/Users/lucyhorowitz/Documents/spaCy-Obsidian project/nlab/nlab-people.txt\",'r').readlines()]\n",
    "names = []\n",
    "for person in people:\n",
    "    for name in person.split():\n",
    "        names.append(name)\n",
    "        names.append(name + \"'s\")\n",
    "for line in open(\"/Users/lucyhorowitz/Documents/spaCy-Obsidian project/nlab/nlab_page_names_raw.txt\", \"r\").readlines():\n",
    "    line = line.strip()\n",
    "    if not line in people:\n",
    "        line = line.replace(' - ', '-')\n",
    "        line = line.replace('$', '')\n",
    "        line = line.replace(' ', '_')\n",
    "        line = line[0].upper() + line[1:]\n",
    "        linesplus = [line + cat for cat in wikicats]\n",
    "        exclude = False\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                exclude = True\n",
    "        if not exclude:\n",
    "            linesplus.append(line)\n",
    "        for thing in linesplus:\n",
    "            if map.title_to_id(thing):\n",
    "                url = thing.replace('_', '+')\n",
    "                url = url.replace(\"'\", '%27')\n",
    "                words = url.split('+')\n",
    "                lower_first_letter = True\n",
    "                if words[0] in names:\n",
    "                     lower_first_letter = False\n",
    "                if words[0].split('-')[0] in names:\n",
    "                    lower_first_letter = False\n",
    "                if words[0].split(\"%27\")[0] in names:\n",
    "                    lower_first_letter = False\n",
    "                if words[0].isupper():\n",
    "                    lower_first_letter = False\n",
    "                if lower_first_letter:\n",
    "                    url = url[0].lower() + url[1:]\n",
    "                if \"(\" in url:\n",
    "                    url = url[:url.index(\"(\") - 1]\n",
    "                wiki.write(thing.replace('_', \" \") + \"](https://ncatlab.org/nlab/show/\" + url + '): ' + map.title_to_id(thing)  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOSGOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'atom': 'locally Noetherian scheme', 'gend': ''}\n",
      "{'atom': 'reduced scheme', 'gend': ''}\n",
      "{'atom': 'integral scheme', 'gend': ''}\n",
      "{'atom': 'separated scheme', 'gend': ''}\n",
      "{'atom': 'irreducible scheme', 'gend': ''}\n",
      "{'atom': 'quasi-compact scheme', 'gend': ''}\n",
      "{'atom': 'scheme of finite type', 'gend': ''}\n",
      "{'atom': 'reduced group', 'gend': ''}\n",
      "{'atom': 'finitely presentable category', 'gend': ''}\n",
      "{'atom': 'locally finitely presentable category', 'gend': ''}\n",
      "{'atom': 'projective set', 'gend': ''}\n",
      "{'atom': 'cofinal set', 'gend': ''}\n",
      "{'atom': 'complementary set', 'gend': ''}\n",
      "{'atom': 'smooth manifold', 'gend': ''}\n",
      "{'atom': 'compact manifold', 'gend': ''}\n",
      "{'atom': 'connected manifold', 'gend': ''}\n",
      "{'atom': 'affix', 'gend': ''}\n",
      "{'atom': 'trihedral angle', 'gend': ''}\n",
      "{'atom': 'real axis', 'gend': ''}\n",
      "{'atom': 'imaginary axis', 'gend': ''}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "hosgood = json.load(open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/hosgood.json'))\n",
    "hmaps = open('hosgood-maps.txt', 'a')\n",
    "for term in hosgood:\n",
    "    try:\n",
    "        hmaps.write(hosgood[term][\"EN\"][\"atom\"]+\": \"+hosgood[term][\"refs\"][\"wikidata\"] + \"\\n\")\n",
    "    except:\n",
    "        print(hosgood[term][\"EN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DENNIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "\n",
    "wikicats = ['_(mathematics)', '_(linear_algebra)', '_(algebraic_geometry)', '_(algebraic_topology)', '_(calculus)', '_(category_theory)',\n",
    "             '_(commutative_algebra)', '_(field_theory)', '_(game_theory)', '_(topology)', '_(differential_geometry)', '_(graph_theory)', \n",
    "             '_(group_theory)', '_(invariant_theory)', '_(module_theory)', '_(order_theory)', '_(probability)', '_(ring_theory)',\n",
    "             '_(representation_theory)', '_(set_theory)', '_(string_theory)', '_(symplectic geometry)', '_(tensor_theory)']\n",
    "\n",
    "wiki = open('mathwords-mappings.txt', 'a')\n",
    "#wiki.write('\\nIN LEAN:\\n')\n",
    "for text in open('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/mathwords.txt', 'r').readlines():\n",
    "    line = text.strip()\n",
    "    \n",
    "    line = line.replace(' ', \"_\")\n",
    "    \n",
    "    line = line[0].upper() + line[1:]\n",
    "    #print(line)\n",
    "    linesplus = [line + cat for cat in wikicats]\n",
    "    exclude = False\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            exclude = True\n",
    "    if not exclude:\n",
    "        linesplus.append(line)\n",
    "    for thing in linesplus:\n",
    "        if map.title_to_id(thing):\n",
    "            wiki.write(thing.replace('_', \" \") + ': ' + map.title_to_id(thing)  + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q654302'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map = WikiMapper('/Users/lucyhorowitz/Documents/spaCy-Obsidian project/wikidata/index_enwiki-20190420.db')\n",
    "map.title_to_id(\"Group\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
